{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import keras as k\n",
    "from keras import callbacks\n",
    "from keras import backend as KB\n",
    "from keras.engine import Layer\n",
    "from keras.layers import Activation\n",
    "from keras.layers import LeakyReLU, Dense, Input, Embedding, Dropout, Reshape\n",
    "from keras.layers import Bidirectional, GRU, Flatten, SpatialDropout1D, Conv1D\n",
    "from keras.datasets import imdb\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Model\n",
    "from keras.utils import to_categorical\n",
    "from common import vocabulary, utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow version: 1.6.0\n",
      "Keras version: 2.1.5\n"
     ]
    }
   ],
   "source": [
    "print(\"Tensorflow version:\", tf.__version__)\n",
    "print(\"Keras version:\", k.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First attempt\n",
    "x_train: sentence - list of words  \n",
    "y_train: sentence - list of NER tag\n",
    "\n",
    "### Second attempt (maybe?)\n",
    "x_train: sentence - list of words, POS tags  \n",
    "y_train: sentence - list of NER tags\n",
    "\n",
    "### Notes\n",
    "> Does it makes sense to use 'O' as the padding for the NER tags? Should we use something else?\n",
    "> - Currently, I am using `<s>` and `</s>`  \n",
    "\n",
    "> perhaps we want to try without the I-, B- modifiers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TRAIN_FILE = \"./data/conll2003/eng.train\"\n",
    "DEV_FILE = \"./data/conll2003/eng.testa\"\n",
    "TEST_FILE = \"./data/conll2003/eng.testb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-DOCSTART- -X- -X- O\r\n",
      "\r\n",
      "EU NNP I-NP I-ORG\r\n",
      "rejects VBZ I-VP O\r\n",
      "German JJ I-NP I-MISC\r\n",
      "call NN I-NP O\r\n",
      "to TO I-VP O\r\n",
      "boycott VB I-VP O\r\n",
      "British JJ I-NP I-MISC\r\n",
      "lamb NN I-NP O\r\n"
     ]
    }
   ],
   "source": [
    "!head -10 {TRAIN_FILE}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this is odd that the B-PER tag doesn't appear... investigate this.\n",
    "!grep \"B-PER\" {TRAIN_FILE}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def readfile( filename, pos=False):\n",
    "    '''\n",
    "    read the conll2003 file\n",
    "    \n",
    "    filename(string) - path to conll2003 file (train, test, etc.)\n",
    "    pos(boolean) - flag if true will include pos tags in returned list\n",
    "    returns a list of lists of lists corresponding to the words in each sentence\n",
    "    \n",
    "    '''\n",
    "    f = open(filename)\n",
    "    sentences = []\n",
    "    sentence = []\n",
    "    for line in f:\n",
    "        if len(line) == 0 or line.startswith('-DOCSTART') or line[0] == \"\\n\":\n",
    "            if len(sentence) > 0:\n",
    "                sentences.append(sentence)\n",
    "                sentence = []\n",
    "            continue\n",
    "        splits = line.strip().split(' ')\n",
    "        word = [splits[0], splits[1], splits[-1]] if pos else [splits[0], splits[-1]]\n",
    "        sentence.append( word)\n",
    "\n",
    "    if len(sentence) > 0:\n",
    "        sentences.append(sentence)\n",
    "        sentence = []\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainSentences = readfile(TRAIN_FILE, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['EU', 'NNP', 'I-ORG'], ['rejects', 'VBZ', 'O'], ['German', 'JJ', 'I-MISC'], ['call', 'NN', 'O'], ['to', 'TO', 'O'], ['boycott', 'VB', 'O'], ['British', 'JJ', 'I-MISC'], ['lamb', 'NN', 'O'], ['.', '.', 'O']]\n",
      "[['Peter', 'NNP', 'I-PER'], ['Blackburn', 'NNP', 'I-PER']]\n",
      "[['BRUSSELS', 'NNP', 'I-LOC'], ['1996-08-22', 'CD', 'O']]\n",
      "[['The', 'DT', 'O'], ['European', 'NNP', 'I-ORG'], ['Commission', 'NNP', 'I-ORG'], ['said', 'VBD', 'O'], ['on', 'IN', 'O'], ['Thursday', 'NNP', 'O'], ['it', 'PRP', 'O'], ['disagreed', 'VBD', 'O'], ['with', 'IN', 'O'], ['German', 'JJ', 'I-MISC'], ['advice', 'NN', 'O'], ['to', 'TO', 'O'], ['consumers', 'NNS', 'O'], ['to', 'TO', 'O'], ['shun', 'VB', 'O'], ['British', 'JJ', 'I-MISC'], ['lamb', 'NN', 'O'], ['until', 'IN', 'O'], ['scientists', 'NNS', 'O'], ['determine', 'VBP', 'O'], ['whether', 'IN', 'O'], ['mad', 'JJ', 'O'], ['cow', 'NN', 'O'], ['disease', 'NN', 'O'], ['can', 'MD', 'O'], ['be', 'VB', 'O'], ['transmitted', 'VBN', 'O'], ['to', 'TO', 'O'], ['sheep', 'NN', 'O'], ['.', '.', 'O']]\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, 4):\n",
    "    print(trainSentences[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determine the max sentence length\n",
    "> Let's start by clipping the longest 5%  \n",
    "> This will probably change, but we've got to start somewhere."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainLens = np.array([ len(s) for s in trainSentences])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAEICAYAAABWJCMKAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nmVlJOQGYmZWUE4CZWUn9f7Kds66EYRhVAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f8d48442470>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clip sentences longer than 37.0 words\n"
     ]
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "clipPct = 5\n",
    "\n",
    "maxLen = np.percentile(trainLens, 100 - clipPct)\n",
    "histLens = plt.hist(trainLens, bins=30)\n",
    "plt.vlines( maxLen, 0, max(histLens[0]), linestyles=\"dashed\")\n",
    "plt.title(\"Sentence lengths in corpus\")\n",
    "plt.show()\n",
    "print( \"Clip sentences longer than\", maxLen, \"words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([  2.66500000e+03,   3.33600000e+03,   2.12800000e+03,\n",
      "         9.26000000e+02,   8.61000000e+02,   8.54000000e+02,\n",
      "         8.58000000e+02,   6.27000000e+02,   7.23000000e+02,\n",
      "         4.59000000e+02,   3.25000000e+02,   1.37000000e+02,\n",
      "         7.90000000e+01,   3.90000000e+01,   1.00000000e+01,\n",
      "         9.00000000e+00,   2.00000000e+00,   1.00000000e+00,\n",
      "         0.00000000e+00,   0.00000000e+00,   1.00000000e+00,\n",
      "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
      "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
      "         0.00000000e+00,   0.00000000e+00,   1.00000000e+00]), array([   1.        ,    4.73333333,    8.46666667,   12.2       ,\n",
      "         15.93333333,   19.66666667,   23.4       ,   27.13333333,\n",
      "         30.86666667,   34.6       ,   38.33333333,   42.06666667,\n",
      "         45.8       ,   49.53333333,   53.26666667,   57.        ,\n",
      "         60.73333333,   64.46666667,   68.2       ,   71.93333333,\n",
      "         75.66666667,   79.4       ,   83.13333333,   86.86666667,\n",
      "         90.6       ,   94.33333333,   98.06666667,  101.8       ,\n",
      "        105.53333333,  109.26666667,  113.        ]), <a list of 30 Patch objects>)\n",
      "Max sentence length:  113\n"
     ]
    }
   ],
   "source": [
    "print(histLens)\n",
    "print( \"Max sentence length: \", max(trainLens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[['EU', 'NNP', 'I-ORG'],\n",
       "  ['rejects', 'VBZ', 'O'],\n",
       "  ['German', 'JJ', 'I-MISC'],\n",
       "  ['call', 'NN', 'O'],\n",
       "  ['to', 'TO', 'O'],\n",
       "  ['boycott', 'VB', 'O'],\n",
       "  ['British', 'JJ', 'I-MISC'],\n",
       "  ['lamb', 'NN', 'O'],\n",
       "  ['.', '.', 'O']],\n",
       " [['Peter', 'NNP', 'I-PER'], ['Blackburn', 'NNP', 'I-PER']],\n",
       " [['BRUSSELS', 'NNP', 'I-LOC'], ['1996-08-22', 'CD', 'O']],\n",
       " [['The', 'DT', 'O'],\n",
       "  ['European', 'NNP', 'I-ORG'],\n",
       "  ['Commission', 'NNP', 'I-ORG'],\n",
       "  ['said', 'VBD', 'O'],\n",
       "  ['on', 'IN', 'O'],\n",
       "  ['Thursday', 'NNP', 'O'],\n",
       "  ['it', 'PRP', 'O'],\n",
       "  ['disagreed', 'VBD', 'O'],\n",
       "  ['with', 'IN', 'O'],\n",
       "  ['German', 'JJ', 'I-MISC'],\n",
       "  ['advice', 'NN', 'O'],\n",
       "  ['to', 'TO', 'O'],\n",
       "  ['consumers', 'NNS', 'O'],\n",
       "  ['to', 'TO', 'O'],\n",
       "  ['shun', 'VB', 'O'],\n",
       "  ['British', 'JJ', 'I-MISC'],\n",
       "  ['lamb', 'NN', 'O'],\n",
       "  ['until', 'IN', 'O'],\n",
       "  ['scientists', 'NNS', 'O'],\n",
       "  ['determine', 'VBP', 'O'],\n",
       "  ['whether', 'IN', 'O'],\n",
       "  ['mad', 'JJ', 'O'],\n",
       "  ['cow', 'NN', 'O'],\n",
       "  ['disease', 'NN', 'O'],\n",
       "  ['can', 'MD', 'O'],\n",
       "  ['be', 'VB', 'O'],\n",
       "  ['transmitted', 'VBN', 'O'],\n",
       "  ['to', 'TO', 'O'],\n",
       "  ['sheep', 'NN', 'O'],\n",
       "  ['.', '.', 'O']],\n",
       " [['Germany', 'NNP', 'I-LOC'],\n",
       "  [\"'s\", 'POS', 'O'],\n",
       "  ['representative', 'NN', 'O'],\n",
       "  ['to', 'TO', 'O'],\n",
       "  ['the', 'DT', 'O'],\n",
       "  ['European', 'NNP', 'I-ORG'],\n",
       "  ['Union', 'NNP', 'I-ORG'],\n",
       "  [\"'s\", 'POS', 'O'],\n",
       "  ['veterinary', 'JJ', 'O'],\n",
       "  ['committee', 'NN', 'O'],\n",
       "  ['Werner', 'NNP', 'I-PER'],\n",
       "  ['Zwingmann', 'NNP', 'I-PER'],\n",
       "  ['said', 'VBD', 'O'],\n",
       "  ['on', 'IN', 'O'],\n",
       "  ['Wednesday', 'NNP', 'O'],\n",
       "  ['consumers', 'NNS', 'O'],\n",
       "  ['should', 'MD', 'O'],\n",
       "  ['buy', 'VB', 'O'],\n",
       "  ['sheepmeat', 'NN', 'O'],\n",
       "  ['from', 'IN', 'O'],\n",
       "  ['countries', 'NNS', 'O'],\n",
       "  ['other', 'JJ', 'O'],\n",
       "  ['than', 'IN', 'O'],\n",
       "  ['Britain', 'NNP', 'I-LOC'],\n",
       "  ['until', 'IN', 'O'],\n",
       "  ['the', 'DT', 'O'],\n",
       "  ['scientific', 'JJ', 'O'],\n",
       "  ['advice', 'NN', 'O'],\n",
       "  ['was', 'VBD', 'O'],\n",
       "  ['clearer', 'JJR', 'O'],\n",
       "  ['.', '.', 'O']]]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainSentences[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['beverage', '55,000-B', 'sheepmeat', 'clinched', 'Lye', 'Atlanta-based', 'ate', 'Outlook', 'CVG', 'Mamiit', 'AUG', 'differs', 'Citterio', 'Sussex', 'Return', 'corrected', 'Hafez', 'troubles', 'redraw', 'tightened']\n",
      "Vocab size: 23626\n",
      "['JJS', 'VB', 'WRB', 'RBR', 'VBD', ')', 'POS', 'JJR', 'EX', '.', 'TO', 'DT', 'NN', 'NNP', 'RBS', '<s>', ',', 'UH', 'RP', '<unk>', 'RB', 'VBN', '$', '(', 'CC', 'VBZ', ':', '\"', 'VBP', 'MD', 'WP', 'WP$', 'JJ', 'LS', 'NN|SYM', 'PDT', 'IN', 'SYM', 'PRP$', '</s>', 'CD', 'WDT', 'VBG', 'NNPS', 'NNS', 'PRP', \"''\", 'FW']\n",
      "['B-MISC', '<s>', 'O', '<unk>', 'I-MISC', 'B-LOC', 'I-PER', 'I-ORG', '</s>', 'I-LOC', 'B-ORG']\n"
     ]
    }
   ],
   "source": [
    "# build vocabulary - thank you w266\n",
    "# -- first attempt, leave in all numbers and maintain case\n",
    "flatData = [w for w in zip(*utils.flatten(trainSentences))]\n",
    "\n",
    "# try with lower vocab sizes... 10k, 15k, 20k\n",
    "vocab = vocabulary.Vocabulary( flatData[0])\n",
    "posTags = vocabulary.Vocabulary( flatData[1])\n",
    "nerTags = vocabulary.Vocabulary( flatData[2])\n",
    "\n",
    "print( (list(vocab.wordset)[:20]))\n",
    "print( \"Vocab size:\", vocab.size)\n",
    "print( (list(posTags.wordset)))\n",
    "print( (list(nerTags.wordset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This might be a good place to do some EDA on the vocab objects...\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Done playing around, define the data class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class conll2003Data(object):\n",
    "    \"\"\"\n",
    "    Keep track of data and processing operations for a single CoNLL2003 data file.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, filePath):\n",
    "        \"\"\"\n",
    "        filePath(string): path to a CoNLL2003 raw data file for training the vocabulary\n",
    "        \"\"\"\n",
    "        # vocabulary objects for easy lookup\n",
    "        self.vocab = []\n",
    "        self.posTags = []\n",
    "        self.nerTags = []\n",
    "        \n",
    "        # read in training data\n",
    "        self.sentences = self.readFile(filePath)\n",
    "        \n",
    "    \n",
    "    def readFile( self, filePath):\n",
    "        \"\"\"\n",
    "        Read the conll2003 raw data file\n",
    "\n",
    "        filename(string) - path to conll2003 file (train, test, etc.)\n",
    "        \n",
    "        Returns: a list of lists of lists corresponding to the words, pos tags, and ner tags\n",
    "                 in each sentence\n",
    "\n",
    "        \"\"\"\n",
    "        f = open(filePath)\n",
    "        sentences = []\n",
    "        sentence = []\n",
    "        for line in f:\n",
    "            if len(line) == 0 or line.startswith(\"-DOCSTART\") or line[0] == '\\n':\n",
    "                if len(sentence) > 0:\n",
    "                    sentences.append(sentence)\n",
    "                    sentence = []\n",
    "                continue\n",
    "            \n",
    "            # input format is [ word, pos tag, chunck tag, ner tag]\n",
    "            # we are ignoring the chunck tag\n",
    "            splits = line.strip().split(' ')\n",
    "            word = [splits[0], splits[1], splits[3]]\n",
    "            sentence.append( word)\n",
    "        \n",
    "        # don't forget the last one\n",
    "        if len(sentence) > 0:\n",
    "            sentences.append(sentence)\n",
    "            sentence = []\n",
    "        \n",
    "        return sentences\n",
    "    \n",
    "    def buildVocab( self, vocabSize=None, verbose=False):\n",
    "        \"\"\"\n",
    "        Builds the vocabulary based on the initial data file\n",
    "        \n",
    "        vocabSize(int, default: None-all words) - max number of words to use for vocabulary\n",
    "                                                  (only used for training)\n",
    "        verbose(boolean, default: False)        - print extra info\n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "        flatData = [w for w in zip(*utils.flatten(self.sentences))]\n",
    "        self.vocab = vocabulary.Vocabulary( flatData[0], size=vocabSize)\n",
    "        \n",
    "        # remember these vocabs will have the <s>, </s>, and <unk> tags in there\n",
    "        # sizes need to be interpreted \"-3\" - consider replacing...\n",
    "        self.posTags = vocabulary.Vocabulary( flatData[1])\n",
    "        self.nerTags = vocabulary.Vocabulary( flatData[2])\n",
    "        \n",
    "        if verbose is True:\n",
    "            print( list(self.vocab.wordset)[:5], \"\\n\")\n",
    "            print( list(self.posTags.wordset)[:5], \"\\n\")\n",
    "            print( list(self.nerTags.wordset)[:5], \"\\n\")\n",
    "    \n",
    "    def formatPaddedData( self, sentences, clipPct = 5, verbose=False):\n",
    "        \"\"\"\n",
    "        Format the raw data by padding up to a max sentence length to be usable for training.\n",
    "        Make sure to call buildVocab first.\n",
    "        \n",
    "        sentences(list of lists of lists) - raw data from the CoNLL2003 dataset\n",
    "        clipPct(int, default: 5)          - the percentage of sentences to clip when determining the\n",
    "                                            max length to allow\n",
    "        verbose(boolean, default: False)  - print extra info\n",
    "        \n",
    "        Returns: 3 lists:   vocabulary converted to IDs, \n",
    "                            POS tags converted to IDs,\n",
    "                            NER label tag converted to IDs\n",
    "        \"\"\"\n",
    "        # get the maximum sentence length to clip down to (and pad up to) (add 2 for <s> and </s>)\n",
    "        self.maxSentLen = int(np.percentile( np.array([ len(s) for s in self.sentences]), 100.0 - clipPct)) + 2\n",
    "        \n",
    "        # we have a list of lists (sentences) of lists ([word, posTag, nerTag])\n",
    "        # parse through, pad each sentence with open and close tags, then convert to IDs\n",
    "        vocabIDs = [ self.vocab.words_to_ids( self.vocab.pad_sentence([ word[0] for word in sent])) \\\n",
    "                     for sent in sentences]\n",
    "        posIDs = [ self.posTags.words_to_ids( self.posTags.pad_sentence([word[1] for word in sent])) \\\n",
    "                   for sent in sentences]\n",
    "        nerIDs = [ self.nerTags.words_to_ids( ['<s>'] + [word[2] for word in sent] + ['</s>']) \\\n",
    "                   for sent in sentences]\n",
    "        \n",
    "        if verbose is True:\n",
    "            print( vocabIDs[:5], \"\\n\")\n",
    "            print( posIDs[:5], \"\\n\")\n",
    "            print( nerIDs[:5], \"\\n\")\n",
    "\n",
    "        # pad and clip so all sentences are the same length\n",
    "        vocabIDs = sequence.pad_sequences( vocabIDs, maxlen = self.maxSentLen)\n",
    "        posIDs = sequence.pad_sequences( posIDs, maxlen = self.maxSentLen)\n",
    "        nerIDs = sequence.pad_sequences( nerIDs, maxlen = self.maxSentLen, \n",
    "                                         value=self.nerTags.word_to_id['O'])\n",
    "        \n",
    "        return vocabIDs, posIDs, nerIDs\n",
    "    \n",
    "    #\n",
    "    # I think it makes the most sense to generate all the training data up front.\n",
    "    # If we had more data or planned to augment on the fly, it would make more sense to \n",
    "    # use a generator.\n",
    "    # \n",
    "    # window generation notes\n",
    "    #     Don't cross sentence boundaries.\n",
    "    #     This means that each sentence will be padded with (windowLength // 2) open/close\n",
    "    #     tags on each end. Also, when we hit a <s> tag after a </s> tag, start the new\n",
    "    #     window there rather than continuing to slide.\n",
    "    # \n",
    "    def formatWindowedData( self, sentences, windowLength=9, verbose=False):\n",
    "        \"\"\"\n",
    "        Format the raw data by blocking it into context windows of a fixed length corresponding \n",
    "        to the single target NER tag of the central word.\n",
    "        Make sure to call buildVocab first.\n",
    "        \n",
    "        sentences(list of lists of lists) - raw data from the CoNLL2003 dataset\n",
    "        windowLength(int, default: 9)     - The length of the context window\n",
    "                    NOTE - windowLength must be odd to have a central word. If itsn't, 1 will be added.\n",
    "        verbose(boolean, default: False)  - print extra info\n",
    "        \n",
    "        Returns: 3 numpy arrays: vocabulary training data windowed and converted to IDs, \n",
    "                                 POS tags windowed and converted to IDs,\n",
    "                                 NER label tags converted to IDs\n",
    "        \"\"\"\n",
    "        pads = windowLength // 2\n",
    "        \n",
    "        # we have a list of lists (sentences) of lists ([word, posTag, nerTag])\n",
    "        # parse through, pad each sentence with pads open and close tags, then convert to IDs\n",
    "        vocabIDs = [ self.vocab.words_to_ids( [\"<s>\"] * pads + [word[0] for word in sent] + [\"</s>\"] * pads) \\\n",
    "                     for sent in sentences]\n",
    "        posIDs = [ self.posTags.words_to_ids( [\"<s>\"] * pads + [word[1] for word in sent] + [\"</s>\"] * pads) \\\n",
    "                   for sent in sentences]\n",
    "        nerIDs = [ self.nerTags.words_to_ids( [\"<s>\"] * pads + [word[2] for word in sent] + [\"</s>\"] * pads) \\\n",
    "                   for sent in sentences]\n",
    "        \n",
    "        if verbose is True:\n",
    "            print( vocabIDs[:5], \"\\n\")\n",
    "            print( posIDs[:5], \"\\n\")\n",
    "            print( nerIDs[:5], \"\\n\")\n",
    "        \n",
    "        assert(len(vocabIDs) == len(posIDs) and len(posIDs) == len(nerIDs))\n",
    "        \n",
    "        # build the data to train on by sliding the window across each sentence\n",
    "        # at this point, all 3 lists are the same size, so we can run through them all at once\n",
    "        featsVocab, featsPOS, featsNER = [], [], []\n",
    "        for sentID in range( len(vocabIDs)):\n",
    "            sent = vocabIDs[sentID]\n",
    "            sentPOS = posIDs[sentID]\n",
    "            sentNER = nerIDs[sentID]\n",
    "            \n",
    "            for ID in range( len(sent) - windowLength + 1):\n",
    "                featsVocab.append( sent[ID:ID + windowLength])\n",
    "                featsPOS.append( sentPOS[ID:ID + windowLength])\n",
    "                featsNER.append( sentNER[ID + windowLength // 2])\n",
    "        \n",
    "        return np.array(featsVocab), np.array(featsPOS), np.array(featsNER)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the windowing functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentences = trainSentences\n",
    "windowLength = 9\n",
    "pads = windowLength // 2\n",
    "verbose = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 0, 0, 0, 959, 11985, 235, 764, 8, 4149, 211, 6184, 3, 1, 1, 1, 1], [0, 0, 0, 0, 734, 2070, 1, 1, 1, 1], [0, 0, 0, 0, 1381, 136, 1, 1, 1, 1]] \n",
      "\n",
      "[[0, 0, 0, 0, 3, 22, 8, 4, 17, 13, 8, 4, 11, 1, 1, 1, 1], [0, 0, 0, 0, 3, 3, 1, 1, 1, 1], [0, 0, 0, 0, 3, 5, 1, 1, 1, 1]] \n",
      "\n",
      "[[0, 0, 0, 0, 5, 3, 7, 3, 3, 3, 7, 3, 3, 1, 1, 1, 1], [0, 0, 0, 0, 4, 4, 1, 1, 1, 1], [0, 0, 0, 0, 6, 3, 1, 1, 1, 1]] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# we have a list of lists (sentences) of lists ([word, posTag, nerTag])\n",
    "# parse through, pad each sentence with open and close tags, then convert to IDs\n",
    "vocabIDs = [ vocab.words_to_ids( [\"<s>\"] * pads + [word[0] for word in sent] + [\"</s>\"] * pads) \\\n",
    "             for sent in sentences]\n",
    "posIDs = [ posTags.words_to_ids( [\"<s>\"] * pads + [word[1] for word in sent] + [\"</s>\"] * pads) \\\n",
    "           for sent in sentences]\n",
    "nerIDs = [ nerTags.words_to_ids( [\"<s>\"] * pads + [word[2] for word in sent] + [\"</s>\"] * pads) \\\n",
    "           for sent in sentences]\n",
    "\n",
    "if verbose is True:\n",
    "    print( vocabIDs[:3], \"\\n\")\n",
    "    print( posIDs[:3], \"\\n\")\n",
    "    print( nerIDs[:3], \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 959, 11985, 235, 764, 8, 4149, 211, 6184, 3, 1, 1, 1, 1]\n",
      "[0, 0, 0, 0, 3, 22, 8, 4, 17, 13, 8, 4, 11, 1, 1, 1, 1]\n",
      "[0, 0, 0, 0, 5, 3, 7, 3, 3, 3, 7, 3, 3, 1, 1, 1, 1] \n",
      "\n",
      "[0, 0, 0, 0, 734, 2070, 1, 1, 1, 1]\n",
      "[0, 0, 0, 0, 3, 3, 1, 1, 1, 1]\n",
      "[0, 0, 0, 0, 4, 4, 1, 1, 1, 1]\n",
      "\n",
      "\n",
      "[0, 0, 0, 0, 959, 11985, 235, 764, 8]\n",
      "[0, 0, 0, 0, 3, 22, 8, 4, 17]\n",
      "Center NER tag: 5\n",
      "Center ID: 959 \n",
      "\n",
      "[0, 0, 0, 959, 11985, 235, 764, 8, 4149]\n",
      "[0, 0, 0, 3, 22, 8, 4, 17, 13]\n",
      "Center NER tag: 3\n",
      "Center ID: 11985 \n",
      "\n",
      "[0, 0, 959, 11985, 235, 764, 8, 4149, 211]\n",
      "[0, 0, 3, 22, 8, 4, 17, 13, 8]\n",
      "Center NER tag: 7\n",
      "Center ID: 235 \n",
      "\n",
      "[0, 959, 11985, 235, 764, 8, 4149, 211, 6184]\n",
      "[0, 3, 22, 8, 4, 17, 13, 8, 4]\n",
      "Center NER tag: 3\n",
      "Center ID: 764 \n",
      "\n",
      "[959, 11985, 235, 764, 8, 4149, 211, 6184, 3]\n",
      "[3, 22, 8, 4, 17, 13, 8, 4, 11]\n",
      "Center NER tag: 3\n",
      "Center ID: 8 \n",
      "\n",
      "[11985, 235, 764, 8, 4149, 211, 6184, 3, 1]\n",
      "[22, 8, 4, 17, 13, 8, 4, 11, 1]\n",
      "Center NER tag: 3\n",
      "Center ID: 4149 \n",
      "\n",
      "[235, 764, 8, 4149, 211, 6184, 3, 1, 1]\n",
      "[8, 4, 17, 13, 8, 4, 11, 1, 1]\n",
      "Center NER tag: 7\n",
      "Center ID: 211 \n",
      "\n",
      "[764, 8, 4149, 211, 6184, 3, 1, 1, 1]\n",
      "[4, 17, 13, 8, 4, 11, 1, 1, 1]\n",
      "Center NER tag: 3\n",
      "Center ID: 6184 \n",
      "\n",
      "[8, 4149, 211, 6184, 3, 1, 1, 1, 1]\n",
      "[17, 13, 8, 4, 11, 1, 1, 1, 1]\n",
      "Center NER tag: 3\n",
      "Center ID: 3 \n",
      "\n",
      "[0, 0, 0, 0, 734, 2070, 1, 1, 1]\n",
      "[0, 0, 0, 0, 3, 3, 1, 1, 1]\n",
      "Center NER tag: 4\n",
      "Center ID: 734 \n",
      "\n",
      "[0, 0, 0, 734, 2070, 1, 1, 1, 1]\n",
      "[0, 0, 0, 3, 3, 1, 1, 1, 1]\n",
      "Center NER tag: 4\n",
      "Center ID: 2070 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# build the data to train on by sliding the window across each sentence\n",
    "# at this point, all 3 lists are the same size, so we can run through them all at once\n",
    "featsVocab, featsPOS, featsNER = [], [], []\n",
    "for sentID in range( len(vocabIDs)):\n",
    "    sent = vocabIDs[sentID]\n",
    "    sentPOS = posIDs[sentID]\n",
    "    sentNER = nerIDs[sentID]\n",
    "    for ID in range( len(sent) - windowLength + 1):\n",
    "        featsVocab.append( sent[ID:ID + windowLength])\n",
    "        featsPOS.append( sentPOS[ID:ID + windowLength])\n",
    "        featsNER.append( sentNER[ID + windowLength // 2])\n",
    "\n",
    "showID = 0\n",
    "print( vocabIDs[showID])\n",
    "print( posIDs[showID])\n",
    "print( nerIDs[showID], '\\n')\n",
    "print( vocabIDs[showID+1])\n",
    "print( posIDs[showID+1])\n",
    "print( nerIDs[showID+1])\n",
    "print( '\\n')\n",
    "for i in range(11):\n",
    "    print( featsVocab[i])\n",
    "    print( featsPOS[i])\n",
    "    print( \"Center NER tag:\", featsNER[i])\n",
    "    print( \"Center ID:\", featsVocab[i][windowLength // 2], '\\n')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 0, 0, 0, 959, 11985, 235, 764, 8, 4149, 211, 6184, 3, 1, 1, 1, 1], [0, 0, 0, 0, 734, 2070, 1, 1, 1, 1], [0, 0, 0, 0, 1381, 136, 1, 1, 1, 1], [0, 0, 0, 0, 20, 228, 457, 15, 14, 68, 37, 8129, 26, 235, 4150, 8, 2478, 8, 11986, 211, 6184, 409, 3544, 2071, 501, 1791, 1922, 653, 289, 41, 8130, 8, 1923, 3, 1, 1, 1, 1], [0, 0, 0, 0, 116, 16, 3112, 8, 5, 228, 487, 16, 2752, 1060, 8131, 8132, 15, 14, 73, 2478, 259, 876, 8133, 28, 539, 126, 114, 124, 409, 5, 2479, 4150, 21, 11987, 3, 1, 1, 1, 1]] \n",
      "\n",
      "[[0, 0, 0, 0, 3, 22, 8, 4, 17, 13, 8, 4, 11, 1, 1, 1, 1], [0, 0, 0, 0, 3, 3, 1, 1, 1, 1], [0, 0, 0, 0, 3, 5, 1, 1, 1, 1], [0, 0, 0, 0, 7, 3, 3, 10, 6, 3, 18, 10, 6, 8, 4, 17, 9, 17, 13, 8, 4, 6, 9, 27, 6, 8, 4, 4, 28, 13, 14, 17, 4, 11, 1, 1, 1, 1], [0, 0, 0, 0, 3, 25, 4, 17, 7, 3, 3, 25, 8, 4, 3, 3, 10, 6, 3, 9, 28, 13, 4, 6, 9, 8, 6, 3, 6, 7, 8, 4, 10, 36, 11, 1, 1, 1, 1]] \n",
      "\n",
      "[[0, 0, 0, 0, 5, 3, 7, 3, 3, 3, 7, 3, 3, 1, 1, 1, 1], [0, 0, 0, 0, 4, 4, 1, 1, 1, 1], [0, 0, 0, 0, 6, 3, 1, 1, 1, 1], [0, 0, 0, 0, 3, 5, 5, 3, 3, 3, 3, 3, 3, 7, 3, 3, 3, 3, 3, 7, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 1, 1, 1], [0, 0, 0, 0, 6, 3, 3, 3, 3, 5, 5, 3, 3, 3, 4, 4, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 6, 3, 3, 3, 3, 3, 3, 3, 1, 1, 1, 1]] \n",
      "\n",
      "Windowed sentence length: 9\n",
      "[[    0     0     0     0   959 11985   235   764     8]\n",
      " [    0     0     0   959 11985   235   764     8  4149]\n",
      " [    0     0   959 11985   235   764     8  4149   211]\n",
      " [    0   959 11985   235   764     8  4149   211  6184]\n",
      " [  959 11985   235   764     8  4149   211  6184     3]\n",
      " [11985   235   764     8  4149   211  6184     3     1]\n",
      " [  235   764     8  4149   211  6184     3     1     1]\n",
      " [  764     8  4149   211  6184     3     1     1     1]\n",
      " [    8  4149   211  6184     3     1     1     1     1]\n",
      " [    0     0     0     0   734  2070     1     1     1]] \n",
      " [5 3 7 3 3 3 7 3 3 4]\n"
     ]
    }
   ],
   "source": [
    "#clipPct = 5\n",
    "windowLength = 9\n",
    "\n",
    "# read in training data\n",
    "vocabData = conll2003Data( TRAIN_FILE)\n",
    "\n",
    "# not yet using the pos tags\n",
    "# try with lower vocab sizes... 10k, 15k, 20k\n",
    "vocabData.buildVocab( vocabSize=20000)\n",
    "trainX, _, trainY = vocabData.formatWindowedData( vocabData.sentences, \n",
    "                                                  windowLength=windowLength,\n",
    "                                                  verbose=True)\n",
    "\n",
    "# check the first few windows\n",
    "print( \"Windowed sentence length:\", len(trainX[0]))\n",
    "print( trainX[:10], '\\n', trainY[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 0, 0, 0, 452, 19, 2, 9044, 8349, 128, 7588, 1828, 16059, 10368, 3, 1, 1, 1, 1], [0, 0, 0, 0, 212, 2749, 1, 1, 1, 1], [0, 0, 0, 0, 292, 905, 6372, 3615, 6347, 266, 155, 17, 1373, 14, 93, 32, 2530, 58, 3173, 29, 42, 274, 12, 1732, 371, 7, 54, 281, 8, 255, 72, 25, 5, 549, 6, 5, 2191, 467, 3, 1, 1, 1, 1], [0, 0, 0, 0, 2501, 1209, 14, 426, 4, 1767, 4, 496, 41, 17973, 32, 859, 2761, 1500, 4, 7170, 12, 1820, 129, 519, 7, 14, 302, 215, 2815, 183, 61, 17, 188, 115, 7, 56, 19891, 149, 77, 2531, 3, 1, 1, 1, 1], [0, 0, 0, 0, 932, 5882, 3173, 67, 17, 3338, 14, 5, 739, 551, 25, 2, 5879, 4, 2530, 3589, 56, 49, 274, 29, 6350, 371, 111, 234, 4479, 67, 17, 2, 26, 134, 2, 2841, 10895, 450, 87, 17, 3338, 3, 1, 1, 1, 1]] \n",
      "\n",
      "[[0, 0, 0, 0, 3, 23, 3, 3, 6, 3, 3, 3, 3, 4, 11, 1, 1, 1, 1], [0, 0, 0, 0, 3, 5, 1, 1, 1, 1], [0, 0, 0, 0, 3, 3, 4, 3, 3, 10, 5, 6, 5, 6, 3, 6, 3, 10, 3, 6, 7, 4, 16, 5, 9, 6, 5, 9, 17, 13, 6, 6, 7, 4, 6, 7, 4, 4, 11, 1, 1, 1, 1], [0, 0, 0, 0, 26, 4, 6, 4, 12, 15, 12, 28, 13, 8, 6, 4, 9, 3, 12, 3, 16, 3, 7, 10, 31, 6, 4, 6, 3, 10, 31, 6, 14, 4, 6, 26, 8, 4, 6, 3, 11, 1, 1, 1, 1], [0, 0, 0, 0, 6, 21, 3, 31, 6, 5, 6, 7, 4, 4, 6, 3, 3, 12, 3, 10, 26, 8, 4, 6, 5, 22, 6, 21, 10, 31, 6, 5, 6, 3, 27, 3, 3, 21, 5, 6, 5, 11, 1, 1, 1, 1]] \n",
      "\n",
      "[[0, 0, 0, 0, 3, 3, 5, 3, 3, 3, 3, 3, 3, 3, 3, 1, 1, 1, 1], [0, 0, 0, 0, 6, 3, 1, 1, 1, 1], [0, 0, 0, 0, 7, 7, 3, 4, 4, 3, 3, 3, 3, 3, 3, 3, 5, 3, 5, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 1, 1, 1], [0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 5, 3, 5, 3, 5, 3, 3, 3, 3, 3, 3, 5, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 5, 3, 1, 1, 1, 1], [0, 0, 0, 0, 3, 3, 5, 3, 3, 3, 3, 3, 3, 3, 3, 6, 6, 3, 5, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 6, 3, 4, 4, 3, 3, 3, 3, 3, 1, 1, 1, 1]] \n",
      "\n",
      "Padded sentence length: 9\n",
      "[[    0     0     0     0   452    19     2  9044  8349]\n",
      " [    0     0     0   452    19     2  9044  8349   128]\n",
      " [    0     0   452    19     2  9044  8349   128  7588]\n",
      " [    0   452    19     2  9044  8349   128  7588  1828]\n",
      " [  452    19     2  9044  8349   128  7588  1828 16059]\n",
      " [   19     2  9044  8349   128  7588  1828 16059 10368]\n",
      " [    2  9044  8349   128  7588  1828 16059 10368     3]\n",
      " [ 9044  8349   128  7588  1828 16059 10368     3     1]\n",
      " [ 8349   128  7588  1828 16059 10368     3     1     1]\n",
      " [  128  7588  1828 16059 10368     3     1     1     1]\n",
      " [ 7588  1828 16059 10368     3     1     1     1     1]\n",
      " [    0     0     0     0   212  2749     1     1     1]] \n",
      " [3 3 5 3 3 3 3 3 3 3 3 6]\n"
     ]
    }
   ],
   "source": [
    "# read in dev data\n",
    "devSents = vocabData.readFile( DEV_FILE)\n",
    "\n",
    "# not yet using the pos tags\n",
    "devX, _, devY = vocabData.formatWindowedData( devSents, \n",
    "                                              windowLength=windowLength,\n",
    "                                              verbose=True)\n",
    "\n",
    "print( \"Padded sentence length:\", len(devX[0]))\n",
    "print( devX[:12], '\\n', devY[:12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 0, 0, 0, 127, 19, 2, 2, 2, 1436, 4, 2, 471, 2, 15306, 3, 1, 1, 1, 1], [0, 0, 0, 0, 2, 2, 1, 1, 1, 1], [0, 0, 0, 0, 2, 4, 167, 1124, 12138, 2, 1, 1, 1, 1], [0, 0, 0, 0, 216, 531, 5, 1136, 6, 56, 5300, 206, 859, 26, 9, 9010, 934, 181, 77, 1674, 7, 9, 957, 1772, 467, 149, 14, 93, 3, 1, 1, 1, 1], [0, 0, 0, 0, 108, 202, 1269, 56, 6568, 13066, 186, 7, 5, 81, 149, 6, 5, 139, 4, 6959, 8, 9, 3245, 750, 1088, 8, 2, 2, 3, 1, 1, 1, 1]] \n",
      "\n",
      "[[0, 0, 0, 0, 4, 23, 3, 13, 3, 3, 12, 3, 6, 7, 4, 11, 1, 1, 1, 1], [0, 0, 0, 0, 3, 3, 1, 1, 1, 1], [0, 0, 0, 0, 3, 12, 3, 3, 29, 5, 1, 1, 1, 1], [0, 0, 0, 0, 3, 10, 7, 4, 6, 26, 8, 3, 4, 6, 7, 8, 5, 27, 6, 3, 6, 7, 3, 3, 4, 4, 6, 3, 11, 1, 1, 1, 1], [0, 0, 0, 0, 16, 3, 10, 26, 4, 13, 18, 6, 7, 4, 4, 6, 7, 4, 12, 21, 17, 7, 4, 5, 4, 17, 9, 3, 11, 1, 1, 1, 1]] \n",
      "\n",
      "[[0, 0, 0, 0, 3, 3, 6, 3, 3, 3, 3, 4, 3, 3, 3, 3, 1, 1, 1, 1], [0, 0, 0, 0, 4, 4, 1, 1, 1, 1], [0, 0, 0, 0, 6, 3, 6, 6, 6, 3, 1, 1, 1, 1], [0, 0, 0, 0, 6, 3, 3, 3, 3, 3, 7, 7, 3, 3, 3, 3, 3, 3, 3, 6, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 1, 1, 1], [0, 0, 0, 0, 3, 6, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 6, 3, 1, 1, 1, 1]] \n",
      "\n",
      "Padded sentence length: 9\n",
      "[[    0     0     0     0   127    19     2     2     2]\n",
      " [    0     0     0   127    19     2     2     2  1436]\n",
      " [    0     0   127    19     2     2     2  1436     4]\n",
      " [    0   127    19     2     2     2  1436     4     2]\n",
      " [  127    19     2     2     2  1436     4     2   471]\n",
      " [   19     2     2     2  1436     4     2   471     2]\n",
      " [    2     2     2  1436     4     2   471     2 15306]\n",
      " [    2     2  1436     4     2   471     2 15306     3]\n",
      " [    2  1436     4     2   471     2 15306     3     1]\n",
      " [ 1436     4     2   471     2 15306     3     1     1]\n",
      " [    4     2   471     2 15306     3     1     1     1]\n",
      " [    2   471     2 15306     3     1     1     1     1]] \n",
      " [3 3 6 3 3 3 3 4 3 3 3 3]\n"
     ]
    }
   ],
   "source": [
    "# read in the test data\n",
    "testSents = vocabData.readFile( TEST_FILE)\n",
    "\n",
    "# not yet using the pos tags\n",
    "testX, _, testY = vocabData.formatWindowedData( testSents, \n",
    "                                                windowLength=windowLength,\n",
    "                                                verbose=True)\n",
    "\n",
    "print( \"Padded sentence length:\", len(testX[0]))\n",
    "print( testX[:12], '\\n', testY[:12])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- - -  \n",
    "### Investigate NER tags\n",
    "> We can see that the dev set doesn't have `B-LOG` or `B-ORG`.  \n",
    "\n",
    "- - -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['O', 'I-PER', 'I-ORG', 'I-LOC', 'I-MISC', 'B-MISC', 'B-ORG', 'B-LOC']\n",
      "{'B-MISC', '<s>', 'O', '<unk>', 'I-MISC', 'B-LOC', 'I-PER', 'I-ORG', '</s>', 'I-LOC', 'B-ORG'}\n"
     ]
    }
   ],
   "source": [
    "s = set([i for i in trainY.flatten()])\n",
    "print(vocabData.nerTags.ids_to_words(s))\n",
    "print(vocabData.nerTags.wordset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['O', 'I-PER', 'I-ORG', 'I-LOC', 'I-MISC', 'B-MISC']\n",
      "{'B-MISC', '<s>', 'O', '<unk>', 'I-MISC', 'B-LOC', 'I-PER', 'I-ORG', '</s>', 'I-LOC', 'B-ORG'}\n"
     ]
    }
   ],
   "source": [
    "s = set([i for i in devY.flatten()])\n",
    "print(vocabData.nerTags.ids_to_words(s))\n",
    "print(vocabData.nerTags.wordset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['O', 'I-PER', 'I-ORG', 'I-LOC', 'I-MISC', 'B-MISC', 'B-ORG', 'B-LOC']\n",
      "{'B-MISC', '<s>', 'O', '<unk>', 'I-MISC', 'B-LOC', 'I-PER', 'I-ORG', '</s>', 'I-LOC', 'B-ORG'}\n"
     ]
    }
   ],
   "source": [
    "s = set([i for i in testY.flatten()])\n",
    "print(vocabData.nerTags.ids_to_words(s))\n",
    "print(vocabData.nerTags.wordset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'O': 169578, 'I-PER': 11128, 'I-ORG': 10001, 'I-LOC': 8286, 'I-MISC': 4556, 'B-MISC': 37, 'B-ORG': 24, 'B-LOC': 11})\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "print( vocabData.nerTags.unigram_counts)\n",
    "print( len(vocabData.nerTags.unigram_counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 <s> \t 11\n",
      "1 </s> \t 11\n",
      "3 O \t 169578\n",
      "4 I-PER \t 11128\n",
      "5 I-ORG \t 10001\n",
      "6 I-LOC \t 8286\n",
      "7 I-MISC \t 4556\n",
      "8 B-MISC \t 37\n",
      "9 B-ORG \t 24\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "c = Counter( \"\".join( map( str, trainY.flatten())))\n",
    "for k, v in sorted(c.items(), key=lambda i: i[0]):\n",
    "    print( k, vocabData.nerTags.id_to_word[int(k)], '\\t', v)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 O \t 169578\n",
      "4 I-PER \t 11128\n",
      "5 I-ORG \t 10001\n",
      "6 I-LOC \t 8286\n",
      "7 I-MISC \t 4556\n",
      "8 B-MISC \t 37\n",
      "9 B-ORG \t 24\n",
      "10 B-LOC \t 11\n"
     ]
    }
   ],
   "source": [
    "bc = Counter()\n",
    "a = []\n",
    "flatX = trainX.flatten()\n",
    "flatY = trainY.flatten()\n",
    "for j, i in enumerate(flatY):\n",
    "    if i == 10:\n",
    "        a += [j]\n",
    "        #print(vocabData.vocab.id_to_word[flatX[j]])\n",
    "    bc[int(i)] += 1\n",
    "\n",
    "for k, v in sorted(bc.items(), key=lambda i: i[0]):\n",
    "    print( k, vocabData.nerTags.id_to_word[int(k)], '\\t', v)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1819, 126796, 126806, 126815, 126825, 126834, 126842, 126852, 126861, 126870, 160096]\n",
      "<s> ['to', 'review', 'his', '<s>', '<s>', 'But', 'Fischler', 'agreed']\n",
      "B-LOC ['O', 'O', 'I-LOC', 'I-LOC', 'B-LOC', 'O', 'O', 'I-LOC']\n",
      "lock [',', 'but', 'they', 'could', 'lock', 'horns', 'in', 'the']\n",
      "B-LOC ['O', 'O', 'O', 'I-LOC', 'B-LOC', 'O', 'O', 'O']\n",
      "in ['they', 'could', 'lock', 'horns', 'in', 'the', 'semis', '.']\n",
      "B-LOC ['O', 'O', 'O', 'I-LOC', 'B-LOC', 'O', 'O', 'O']\n",
      "the ['could', 'lock', 'horns', 'in', 'the', 'semis', '.', '</s>']\n",
      "B-LOC ['O', 'O', 'O', 'I-LOC', 'B-LOC', 'O', 'O', 'O']\n",
      ". ['horns', 'in', 'the', 'semis', '.', '</s>', '</s>', 'lock']\n",
      "B-LOC ['O', 'O', 'I-LOC', 'I-LOC', 'B-LOC', 'O', 'O', 'O']\n",
      "</s> ['in', 'the', 'semis', '.', '</s>', '</s>', '</s>', 'horns']\n",
      "B-LOC ['O', 'O', 'O', 'I-LOC', 'B-LOC', 'O', 'O', 'O']\n",
      "</s> ['in', 'the', 'semis', '.', '</s>', '</s>', '</s>', '</s>']\n",
      "B-LOC ['O', 'O', 'O', 'I-LOC', 'B-LOC', 'O', 'O', 'O']\n",
      "Agassi ['<s>', '<s>', 'Olympic', 'champion', 'Agassi', 'meets', 'Karim', '<s>']\n",
      "B-LOC ['O', 'O', 'I-LOC', 'I-LOC', 'B-LOC', 'O', 'O', 'O']\n",
      "meets ['<s>', 'Olympic', 'champion', 'Agassi', 'meets', 'Karim', 'Alami', '<s>']\n",
      "B-LOC ['O', 'O', 'O', 'I-LOC', 'B-LOC', 'O', 'O', 'O']\n",
      "Karim ['Olympic', 'champion', 'Agassi', 'meets', 'Karim', 'Alami', 'of', '<s>']\n",
      "B-LOC ['O', 'O', 'O', 'I-LOC', 'B-LOC', 'O', 'O', 'O']\n",
      "charged ['in', 'custody', 'on', 'Wednesday', 'charged', 'with', 'attempted', 'murder']\n",
      "B-LOC ['O', 'O', 'O', 'I-LOC', 'B-LOC', 'I-LOC', 'O', 'O']\n"
     ]
    }
   ],
   "source": [
    "print(a)\n",
    "for j in a:\n",
    "    print( vocabData.vocab.id_to_word[flatX[j]], vocabData.vocab.ids_to_words(flatX[j-4:j+4]))\n",
    "    print( vocabData.nerTags.id_to_word[flatY[j]], vocabData.nerTags.ids_to_words(flatY[j-4:j+4]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# capsule layers from Xifeng Guo \n",
    "# https://github.com/XifengGuo/CapsNet-Keras\n",
    "from capsulelayers import CapsuleLayer, PrimaryCap, Length, Mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape y from all labels to a row per class\n",
    "trainY_cat = to_categorical(trainY.astype('float32'))\n",
    "# num_classes is required because the dev set doesn't have all the tags represented\n",
    "devY_cat = to_categorical(devY.astype('float32'), num_classes=trainY_cat.shape[1])\n",
    "testY_cat = to_categorical(testY.astype('float32'), num_classes=trainY_cat.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0\n",
      "1 0\n",
      "2 0\n",
      "3 169578\n",
      "4 180706\n",
      "5 190707\n",
      "6 198993\n",
      "7 203549\n",
      "8 203586\n",
      "9 203610\n",
      "10 203621\n",
      "New shape of train: (203621, 11)\n",
      "New shape of dev: (51362, 11)\n",
      "New shape of test: (46435, 11)\n"
     ]
    }
   ],
   "source": [
    "# look at the 1-hot representation\n",
    "totes = 0\n",
    "for n in range(vocabData.nerTags.size):\n",
    "    for i in trainY_cat:\n",
    "        if i[n] == 1:\n",
    "            totes += 1\n",
    "    print(n, totes)\n",
    "\n",
    "print( \"New shape of train:\", trainY_cat.shape)\n",
    "print( \"New shape of dev:\", devY_cat.shape)\n",
    "print( \"New shape of test:\", testY_cat.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We have a few extra tags in the NER vocab for padding and such. We'll shrink the vectors and remove these unnecessary targets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainY_cat = np.array(list(map( lambda i: i[3:], trainY_cat)))\n",
    "devY_cat = np.array(list(map( lambda i: i[3:], devY_cat)))\n",
    "testY_cat = np.array(list(map( lambda i: i[3:], testY_cat)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 169578\n",
      "1 180706\n",
      "2 190707\n",
      "3 198993\n",
      "4 203549\n",
      "5 203586\n",
      "6 203610\n",
      "7 203621\n",
      "New shape of train: (203621, 8)\n",
      "New shape of dev: (51362, 8)\n",
      "New shape of test: (46435, 8)\n"
     ]
    }
   ],
   "source": [
    "# Make sure it worked\n",
    "totes = 0\n",
    "for n in range(trainY_cat.shape[1]):\n",
    "    for i in trainY_cat:\n",
    "        if i[n] == 1:\n",
    "            totes += 1\n",
    "    print(n, totes)\n",
    "\n",
    "print( \"New shape of train:\", trainY_cat.shape)\n",
    "print( \"New shape of dev:\", devY_cat.shape)\n",
    "print( \"New shape of test:\", testY_cat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20000"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabData.vocab.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define hyperparameters\n",
    "max_features = vocabData.vocab.size\n",
    "maxlen = trainX.shape[1]\n",
    "ner_classes = trainY_cat.shape[1]\n",
    "embed_dim = 50\n",
    "num_routing = 1\n",
    "\n",
    "save_dir = './result'\n",
    "batch_size = 100\n",
    "debug = 2\n",
    "epochs = 1\n",
    "dropout_p = 0.25\n",
    "lam_recon = 0.0005\n",
    "\n",
    "#Load train and test data\n",
    "#(x_train, y_train), (x_test, y_test) = reuters.load_data(num_words=max_features)\n",
    "#x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "#x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embed (?, 9, 50, 1)\n",
      "WARNING:tensorflow:From /home/dhuber/capsnet/capsnet_NLP/capsulelayers.py:137: calling softmax (from tensorflow.python.ops.nn_ops) with dim is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "dim is deprecated, use axis instead\n",
      "ner_caps (?, 8, 16)\n",
      "out_caps (?, 8)\n"
     ]
    }
   ],
   "source": [
    "from keras import Sequential\n",
    "#Build embedding and convolutional layers\n",
    "\n",
    "x = Input(shape=(maxlen,))\n",
    "embed = Embedding(max_features, embed_dim, input_length=maxlen)(x)\n",
    "\n",
    "embed = Reshape( ( maxlen, embed_dim, 1))(embed)\n",
    "#k.backend.expand_dims( embed)\n",
    "\n",
    "# maybe add this back in!! - make sure to consider the window length\n",
    "#conv1 = Conv1D(filters=256, kernel_size=3, strides=1, padding='valid', \n",
    "#                      activation='relu', name='conv1')(embed)\n",
    "\n",
    "print( \"embed\", embed.get_shape())\n",
    "    \n",
    "# Layer 2: Conv2D layer with `squash` activation, then reshape to \n",
    "# kernel_size should be smaller than window size... maybe half of window size?\n",
    "# [None, num_capsule, dim_vector]\n",
    "primarycaps = PrimaryCap(embed, dim_capsule=8, n_channels=32, kernel_size=windowLength // 2, \n",
    "                         strides=2, padding='valid')\n",
    "\n",
    "# Layer 3: Capsule layer. Routing algorithm works here.\n",
    "ner_caps = CapsuleLayer(num_capsule=ner_classes, dim_capsule=16, \n",
    "                     routings=num_routing, name='nercaps')(primarycaps)\n",
    "\n",
    "# I worry that we will need (n_class * trainY.shape[2]) capsules here... \n",
    "# 1 capsule for each label for each word in the sentence... maybe this is where the LSTM comes in?\n",
    "\n",
    "# Layer 4: This is an auxiliary layer to replace each capsule with its length. \n",
    "# Just to match the true label's shape.\n",
    "# If using tensorflow, this will not be necessary. :)\n",
    "print( \"ner_caps\", ner_caps.get_shape())\n",
    "out_caps = Length(name='out_caps')(ner_caps)\n",
    "print( \"out_caps\", out_caps.get_shape())\n",
    "\n",
    "# Decoder network. - probably get the dims right then flatten it\n",
    "y = Input(shape=(trainY_cat.shape[1],))\n",
    "#y = Reshape( ( maxlen * ner_classes,), input_shape=( trainY.shape[1], trainY.shape[2],))\n",
    "#y = Flatten( )(y)\n",
    "#y = KB.reshape( y, ( maxlen * ner_classes,))\n",
    "#print(\"y shape:\", y.)\n",
    "#print(ner_caps.shape[1], ner_caps.shape[2], ner_caps.shape[1] * ner_caps.shape[2])\n",
    "#ner_caps = Reshape((ner_caps.shape[1] * ner_caps.shape[2],))(ner_caps)\n",
    "masked = Mask()([ner_caps, y])  # The true label is used to mask the output of capsule layer.\n",
    "x_recon = Dense(512, activation='relu')(masked)\n",
    "x_recon = Dense(1024, activation='relu')(x_recon)\n",
    "x_recon = Dense(maxlen, activation='sigmoid')(x_recon)\n",
    "# x_recon = layers.Reshape(target_shape=[1], name='out_recon')(x_recon)\n",
    "\n",
    "capsmodel = Model([x, y], [out_caps, x_recon])\n",
    "#capsmodel = Model([y], [x_recon])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 9)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 9, 50)        1000000     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "reshape_1 (Reshape)             (None, 9, 50, 1)     0           embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "primarycap_conv2d (Conv2D)      (None, 3, 24, 256)   4352        reshape_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "primarycap_reshape (Reshape)    (None, 2304, 8)      0           primarycap_conv2d[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "primarycap_squash (Lambda)      (None, 2304, 8)      0           primarycap_reshape[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "nercaps (CapsuleLayer)          (None, 8, 16)        2359296     primarycap_squash[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 8)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "mask_1 (Mask)                   (None, 128)          0           nercaps[0][0]                    \n",
      "                                                                 input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 512)          66048       mask_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 1024)         525312      dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "out_caps (Length)               (None, 8)            0           nercaps[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 9)            9225        dense_2[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 3,964,233\n",
      "Trainable params: 3,964,233\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Saving weights and logging\n",
    "log = callbacks.CSVLogger(save_dir + '/log.csv')\n",
    "tb = callbacks.TensorBoard(log_dir=save_dir + '/tensorboard-logs', \n",
    "                           batch_size=batch_size, histogram_freq=debug)\n",
    "checkpoint = callbacks.ModelCheckpoint(save_dir + '/weights-{epoch:02d}.h5', \n",
    "                                       save_best_only=True, \n",
    "                                       save_weights_only=True, \n",
    "                                       verbose=1)\n",
    "#lr_decay = callbacks.LearningRateScheduler(schedule=lambda epoch: 0.001 * np.exp(-epoch / 10.))\n",
    "\n",
    "#margin_loss\n",
    "def margin_loss(y_true, y_pred):\n",
    "    L = y_true * KB.square(KB.maximum(0., 0.9 - y_pred)) + 0.5 * (1 - y_true) * KB.square(KB.maximum(0., y_pred - 0.1))\n",
    "    return KB.mean(KB.sum(L, 1))\n",
    "\n",
    "capsmodel.summary()\n",
    "\n",
    "#Save a png of the model shapes and flow\n",
    "#plot_model(capsmodel, to_file=save_dir + '/reuters-model.png', show_shapes=True)\n",
    "\n",
    "# compile the model\n",
    "capsmodel.compile(optimizer='adam',\n",
    "              loss=[margin_loss, 'mse'],\n",
    "              loss_weights=[1., lam_recon],\n",
    "              metrics={'out_caps': 'accuracy'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14041, 39, 11)\n",
      "(14041, 429)\n",
      "[[ 0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.]\n",
      " [ 0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.]\n",
      " [ 0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.]]\n",
      "(3250, 39, 9)\n",
      "(3250, 351)\n",
      "[[ 0.  0.  0.  1.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  1.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  1.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  1.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  1.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  1.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  1.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  1.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  1.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  1.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  1.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  1.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  1.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  1.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  1.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  1.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  1.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  1.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  1.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  1.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  1.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  1.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  1.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  1.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  1.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  1.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  1.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  1.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  1.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  1.  0.  0.  0.]\n",
      " [ 0.  0.  0.  1.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  1.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  1.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  1.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  1.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  1.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  1.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  1.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  1.  0.  0.  0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "print(trainY_cat.shape)\n",
    "print(trainY_cat.reshape(-1, trainY_cat.shape[1] * trainY_cat.shape[2]).shape)\n",
    "print(trainY_cat[0])\n",
    "print(devY_cat.shape)\n",
    "print(devY_cat.reshape(-1, devY_cat.shape[1] * devY_cat.shape[2]).shape)\n",
    "print(devY_cat[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 203621 samples, validate on 51362 samples\n",
      "Epoch 1/1\n",
      " 39900/203621 [====>.........................] - ETA: 59:23 - loss: 7327.4051 - out_caps_loss: 0.8100 - dense_3_loss: 14653189.6391 - out_caps_acc: 0.1244"
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "capsmodel.fit([trainX, trainY_cat], \n",
    "              [trainY_cat, trainX], \n",
    "              batch_size=batch_size, \n",
    "              epochs=epochs, \n",
    "              validation_data=[[devX, devY_cat], [devY_cat, devX]], \n",
    "              callbacks=[log, tb, checkpoint], \n",
    "              verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
